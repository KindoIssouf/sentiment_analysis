{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "import sqlite3 as lite\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@dog_crazy http://twitpic.com/5f4jq - Awww so Toro is your baby?? Soo sweet!\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p='../text_mining_class/MyCode/Training.txt'\n",
    "\n",
    "file = open(p,\"r\")\n",
    "headers=[]\n",
    "tony_id=[]\n",
    "sentiment=[]\n",
    "text = []\n",
    "i=0\n",
    "\n",
    "for sent in file:\n",
    "    if i < 1 :\n",
    "        var = sent.split('\\t')\n",
    "        headers.append(var[0])\n",
    "        headers.append(var[1])\n",
    "        headers.append(var[2])\n",
    "        i=i+1\n",
    "    else:\n",
    "        var = sent.split('\\t')\n",
    "        if len(var) >= 3:\n",
    "            tony_id.append(var[0])\n",
    "            sentiment.append(var[1])\n",
    "            text.append(var[2])\n",
    "            \n",
    "text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Emails\n",
    "# text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in text]\n",
    "\n",
    "# # Remove new line characters\n",
    "# text = [re.sub('\\s+', ' ', sent) for sent in text]\n",
    "\n",
    "# # Remove http\n",
    "# #text = [re.sub('^http://\\w', ' ', sent) for sent in text]\n",
    "\n",
    "# # Remove distracting single quotes\n",
    "# text = [re.sub(\"\\'\", \"\", sent) for sent in text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@dog_crazy http://twitpic.com/5f4jq - Awww so Toro is your baby?? Soo '\n",
      " 'sweet!\\n']\n"
     ]
    }
   ],
   "source": [
    "print(text[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# data_words_nostops = remove_stopwords(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_sent =[]\n",
    "negative_sent =[]\n",
    "\n",
    "#split from: https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/model_selection/_split.py#L2073\n",
    "\n",
    "train, test = train_test_split(text,test_size = 0.1)\n",
    "\n",
    "for i in range (len(train)):\n",
    "    if sentiment[i] =='+':\n",
    "        positive_sent.append(text[i])\n",
    "    else:\n",
    "        negative_sent.append(text[i])\n",
    "        \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# positive_sent = remove_stopwords(positive_sent)\n",
    "# negative_sent = remove_stopwords(negative_sent)\n",
    "        \n",
    "        \n",
    "positive_vocab = list(sent_to_words(positive_sent))\n",
    "negative_vocab = list(sent_to_words(negative_sent))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive_sent =[]\n",
    "test_negative_sent =[]\n",
    "test_data_positive_and_neg_binary =[]\n",
    "\n",
    "for i in range (len(test)):\n",
    "    if sentiment[i] =='+':\n",
    "        test_positive_sent.append(text[i])\n",
    "        test_data_positive_and_neg_binary.append(1)\n",
    "    else:\n",
    "        test_negative_sent.append(text[i])\n",
    "        test_data_positive_and_neg_binary.append(0)\n",
    "        \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "test_positive_vocab = list(sent_to_words(test_positive_sent))\n",
    "test_negative_vocab = list(sent_to_words(test_negative_sent))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_positive_and_neg = list(sent_to_words(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'give': True, 'up': True}, 'neg'), ({'mrsangell': True, 'oh': True, 'no': True, 'truly': True, 'sorry': True, 'to': True, 'hear': True, 'that': True}, 'neg'), ({'gah': True, 'tired': True, 'and': True, 'my': True, 'eye': True, 'hurt': True}, 'neg'), ({'cus': True, 'that': True, 'man': True, 'sed': True, 'he': True, 'doesnt': True, 'want': True, 'me': True, 'to': True, 'work': True, 'there': True, 'im': True, 'lazy': True, 'and': True, 'dnt': True, 'no': True, 'anything': True}, 'neg'), ({'gotta': True, 'blow': True, 'out': True, 'then': True, 'flat': True, 'iron': True, 'my': True, 'hair': True, 'that': True, 'might': True, 'take': True, 'while': True}, 'neg')]\n"
     ]
    }
   ],
   "source": [
    "# from : https://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "\n",
    "\n",
    "train_set = negative_features + positive_features\n",
    "\n",
    "print (train_set[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.4226142261422614\n",
      "Negative: 0.5773857738577386\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "\n",
    "result_test =[]\n",
    "result_pos_test =[]\n",
    "\n",
    "for word in test_data_positive_and_neg:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "        result_test.append(0) \n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    "        result_test.append(1) \n",
    "        \n",
    "\n",
    "print('Positive: ' + str(float(pos)/len(test_data_positive_and_neg)))\n",
    "print('Negative: ' + str(float(neg)/len(test_data_positive_and_neg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "p='/Users/youssefkindo/Downloads/text_mining_class/MyCode/sentiment_testfiles/test1_public.txt'\n",
    "\n",
    "file_1 = open(p,\"r\",encoding = \"ISO-8859-1\")\n",
    "headers_1=[]\n",
    "tony_id_1=[]\n",
    "# sentiment=[]\n",
    "text_1 = []\n",
    "i=0\n",
    "\n",
    "for sent in file_1:\n",
    "    if i < 1 :\n",
    "        var = sent.split('XXX')\n",
    "        headers_1.append(var[0])\n",
    "        headers_1.append(var[1])\n",
    "        i=i+1\n",
    "    else:\n",
    "        var = sent.split('XXX')\n",
    "        if len(var) >= 2:\n",
    "            tony_id_1.append(var[0])\n",
    "#             sentiment.append(var[1])\n",
    "            text_1.append(var[1:])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2='/Users/youssefkindo/Downloads/text_mining_class/MyCode/sentiment_testfiles/test2_public.txt'\n",
    "\n",
    "file_2 = open(p2,\"r\",encoding = \"ISO-8859-1\")\n",
    "headers_2=[]\n",
    "tony_id_2=[]\n",
    "# sentiment=[]\n",
    "text_2 = []\n",
    "i=0\n",
    "\n",
    "for sent in file_2:\n",
    "    if i < 1 :\n",
    "        var = sent.split('XXX')\n",
    "        headers_2.append(var[0])\n",
    "        headers_2.append(var[1])\n",
    "        i=i+1\n",
    "    else:\n",
    "        var = sent.split('XXX')\n",
    "        if len(var) >= 2:\n",
    "            tony_id_2.append(var[0])\n",
    "#             sentiment.append(var[1])\n",
    "            text_2.append(var[1:])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.4118\n",
      "Negative: 0.5882\n"
     ]
    }
   ],
   "source": [
    "#prepare--\n",
    "text_2_prepared = sent_to_words(text_2)\n",
    "\n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "\n",
    "result_test_2 =[]\n",
    "result_pos_test =[]\n",
    "\n",
    "for word in text_2_prepared:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "        result_test_2.append(0) \n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    "        result_test_2.append(1) \n",
    "        \n",
    "\n",
    "print('Positive: ' + str(float(pos)/len(text_2)))\n",
    "print('Negative: ' + str(float(neg)/len(text_2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw5=open('/Users/youssefkindo/Downloads/text_mining_class/MyCode/sentiment_testfiles/result_2.txt','w')\n",
    "\n",
    "fw5.write('ID#,' +\"  \"+'sentiment'+\",\\n\")\n",
    "\n",
    "for i in range(len(text_2)):\n",
    "    #name = patent_id[labels[i]]\n",
    "    #print(name)\n",
    "    if result_test_2[i] == 1:\n",
    "        fw5.write(str( tony_id_2[i]) +\", \"+\"+\"+\",\\n\")\n",
    "    else:\n",
    "        fw5.write(str( tony_id_2[i]) +\", \"+\"-\"+\",\\n\")\n",
    "        \n",
    "    \n",
    "fw5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.4107741077410774\n",
      "Negative: 0.5892258922589226\n"
     ]
    }
   ],
   "source": [
    "#prepare--\n",
    "text_1_prepared = sent_to_words(text_1)\n",
    "\n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "\n",
    "result_test_1 =[]\n",
    "\n",
    "for word in text_1_prepared:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "        result_test_1.append(0) \n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    "        result_test_1.append(1) \n",
    "        \n",
    "\n",
    "print('Positive: ' + str(float(pos)/len(text_1)))\n",
    "print('Negative: ' + str(float(neg)/len(text_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw5=open('/Users/youssefkindo/Downloads/text_mining_class/MyCode/sentiment_testfiles/result_1.txt','w')\n",
    "\n",
    "fw5.write('ID#,' +\"  \"+'sentiment'+\",\\n\")\n",
    "\n",
    "for i in range(len(text_1)):\n",
    "    #name = patent_id[labels[i]]\n",
    "    #print(name)\n",
    "    if result_test_1[i] == 1:\n",
    "        fw5.write(str( tony_id_1[i]) +\", \"+\"+\"+\",\\n\")\n",
    "    else:\n",
    "        fw5.write(str( tony_id_1[i]) +\", \"+\"-\"+\",\\n\")\n",
    "        \n",
    "    \n",
    "fw5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(test_data_positive_and_neg_binary))\n",
    "# print(len(result_test))\n",
    "\n",
    "# print(test_data_positive_and_neg_binary[990:1000])\n",
    "# print(result_test[990:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "# y_true = test_data_positive_and_neg_binary\n",
    "# y_true =[]\n",
    "# l=len(test_negative_vocab)\n",
    "# for i in range (l):\n",
    "#     y_true.append(0)\n",
    "\n",
    "# g=len(test_positive_vocab)\n",
    "# for i in range (g):\n",
    "#     y_true.append(1)\n",
    "\n",
    "    \n",
    "# y_pred = result_test\n",
    "# y_pred = []\n",
    "# y=len(result_neg_test)\n",
    "# for i in range (y):\n",
    "#     y_pred.append(0)\n",
    "\n",
    "# z=len(result_pos_test)\n",
    "# for i in range (z):\n",
    "#     y_pred.append(1)\n",
    "\n",
    "# k=393\n",
    "# for i in range (k):\n",
    "#     y_pred.append(2)\n",
    "\n",
    "# score = f1_score(test_negative_vocab_w_feat,neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f1_score(y_true, y_pred)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw5=open('/Users/youssefkindo/Downloads/text_mining_class/contest_result.txt','w')\n",
    "\n",
    "nameClustDict = {}\n",
    "nameDict = {}\n",
    "for i in range(len(test_negative_vocab)):\n",
    "    #name = patent_id[labels[i]]\n",
    "    #print(name)\n",
    "    if i == 0:  \n",
    "        fw5.write('id,' +\"  \"+'snetiment'+\",\\n\")\n",
    "    \n",
    "    fw5.write(str( tony_id[i]) +\", \"+str(result_test[i])+\",\\n\")\n",
    "    \n",
    "fw5.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
